{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../src\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "import claim_verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():    \n",
    "#     # Tell PyTorch to use the GPU.    \n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n",
    "#     print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n",
    "\n",
    "# # If we dont have GPU but a CPU, training will take place on CPU instead\n",
    "# else:\n",
    "#     print('No GPU available, using the CPU instead.')\n",
    "#     device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_evidence(parsed_evidence):\n",
    "    if type(parsed_evidence) == str:\n",
    "        return parsed_evidence\n",
    "    else:\n",
    "        return ' '.join([' '.join([' '.join([el if len(el)>2 else '' for el in sl]).strip() for sl in lst]) for lst in parsed_evidence]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    global summarizer\n",
    "    \n",
    "    # truncate input somewhat (due to model max length)\n",
    "    # TODO split context into chunks of max len, concat later?\n",
    "    maxlen = 700\n",
    "    if type(text) == str:\n",
    "        text = ' '.join([txt for txt in text.split(' ')[:maxlen]])\n",
    "    else:\n",
    "        text = [' '.join([txt for txt in subtext.split(' ')[:maxlen]]) for subtext in text]\n",
    "\n",
    "        summarized_text = summarizer(text, min_length=min(10, len(text)), max_length=(min(128, len(text))))\n",
    "    \n",
    "    if type(text) != str:\n",
    "        summarized_text = [txt['summary_text'] for txt in summarized_text]\n",
    "    else:\n",
    "        summarized_text = summarized_text[0]['summary_text']\n",
    "        \n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_evidence(claim, num_results):\n",
    "    try:\n",
    "        evidence = load_evidence(claim)\n",
    "        print('Existing evidence results found locally, loading from disk.')\n",
    "    except:\n",
    "        links = get_evidences.get_top_k_results_from_google(claim, k=num_results)\n",
    "        evidence = [(l, flatten_evidence(get_evidences.get_relevant_text_from_webpage(l))) for l in links]\n",
    "\n",
    "        # Remove empty evidence\n",
    "        evidence = pd.DataFrame([ev for ev in evidence if len(ev[1]) > 0], columns=['source', 'text'])\n",
    "        \n",
    "        # Clean input\n",
    "        evidence['text'] = evidence['text'].apply(lambda x: clean_input(x))\n",
    "    \n",
    "        # Remove duplicates\n",
    "        evidence.drop_duplicates('text', inplace=True)\n",
    "\n",
    "        # Add summarization\n",
    "        evidence['summary'] = summarize_text(list(evidence['text'].values))\n",
    "\n",
    "        # Save for later use\n",
    "        save_evidence(evidence, claim)\n",
    "        \n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conclusions(conclusions):\n",
    "    num_sources = len(conclusions)\n",
    "    \n",
    "    false_score = conclusions.count('false')*-1\n",
    "    unsure_score = conclusions.count('neutral')*-0.1\n",
    "    true_score = conclusions.count('true')*1\n",
    "    \n",
    "    total_score = false_score + unsure_score + true_score\n",
    "    \n",
    "    print(f'Conclusions: {conclusions}')\n",
    "    \n",
    "#     if total_score < -(num_sources / 3):\n",
    "#         print('This claim is very likely to be false.')\n",
    "#     elif total_score < (num_sources / 3):\n",
    "#         print('This claim is probably untrue.')\n",
    "#     else:\n",
    "#         print('This claim is plausible.')\n",
    "\n",
    "    # rescale score to 0 - 1\n",
    "    normalized_score = round((10 + total_score) / 20, 2)\n",
    "    \n",
    "    print(f'The truth score of this claim is {normalized_score} (scaled between 0 and 1)\\n')\n",
    "    \n",
    "    return normalized_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(text):\n",
    "    # Remove newline\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "def split_to_sentences(text):\n",
    "    return sent_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_zero_shot_model(claim, text):\n",
    "    global zero_shot_model\n",
    "    unsure_threshold = 0.4\n",
    "\n",
    "    zeroshot_labels = [f\"The following statement is True: {claim}\",\n",
    "                       f'Not enough information to verify the following statement: {claim}',\n",
    "                       f\"The following statement is False: {claim}\"]\n",
    "\n",
    "    conclusion = zero_shot_model(text, candidate_labels=zeroshot_labels)\n",
    "    parsed_conclusion = conclusion['labels'][0].split(' ')[4].lower().replace(':', '')\n",
    "    if parsed_conclusion == 'true':\n",
    "        result = True\n",
    "    elif parsed_conclusion == 'false':\n",
    "        result = False\n",
    "    else:\n",
    "        result = 'neutral'\n",
    "        \n",
    "    score = conclusion['scores'][0]\n",
    "    \n",
    "    return str(result).lower() if score > unsure_threshold else 'neutral', score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nli_model(claim, text):\n",
    "    global nli_model\n",
    "    label_mapping = ['false', 'true', 'neutral']   # (['contradiction', 'entailment', 'neutral'])\n",
    "\n",
    "    scores = nli_model.predict([(claim, text)])\n",
    "    \n",
    "    #Convert score to label\n",
    "    label = label_mapping[scores.argmax(axis=1)[0]]\n",
    "    confidence = scores[scores.argmax(axis=1)[0]]\n",
    "\n",
    "    return label, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_claim(claim, model_type='nli'):\n",
    "    \n",
    "    evidence = collect_evidence(claim, num_results=10)\n",
    "        \n",
    "    conclusions = []\n",
    "    confidences = []\n",
    "    for i, row in evidence.iterrows():\n",
    "        source, text, summary = row.source, row.text, row.summary\n",
    "        \n",
    "        if model_type == 'zero-shot':\n",
    "            label, confidence = apply_zero_shot_model(claim, summary)\n",
    "        elif model_type == 'nli':\n",
    "            label, confidence = apply_nli_model(claim, summary)\n",
    "        else:\n",
    "            raise ValueError('No valid model type specified.')\n",
    "            \n",
    "        conclusions += [label]\n",
    "        confidences += [confidence]\n",
    "\n",
    "    parse_conclusions(conclusions)\n",
    "    evidence['conclusion'] = conclusions\n",
    "    \n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evidence(evidence, claim):\n",
    "    claim = claim.replace(' ', '').strip()\n",
    "    pd.DataFrame(evidence).to_csv(f\"../data/temp/{claim}.csv\", sep ='\\t')\n",
    "    \n",
    "def load_evidence(claim):\n",
    "    claim = claim.replace(' ', '').strip()\n",
    "    evidence = pd.read_csv(f\"../data/temp/{claim}.csv\", sep ='\\t')\n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply model on several claims"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# nli_model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "# zero_shot_model = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "nli_model = CrossEncoder('MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli')\n",
    "zero_shot_model = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brad pitt married to Britney Spears? (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"brad pitt is to marry with britney spears\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"brad pitt is not married to britney spears\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joe Biden classified documents found? (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "claim = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"Joe Biden never took home any classified documents after leaving the vice-presidency\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 vaccine causes infertility (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"COVID-19 vaccine causes infertility\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"COVID-19 vaccine does not cause infertility\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jan Smit met de US president (fake, but no relevant evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "claim = \"jan smit has met with US president in 2012\"\n",
    "evidence = claim_verification.investigate_claim(claim, model_type='zero-shot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "import get_evidences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = 'COVID-19 vaccine causes infertility'\n",
    "evidence = claim_verification.load_evidence(claim)\n",
    "context = evidence.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_context = summarizer(context, min_length=5, max_length=50)\n",
    "summarized_context = summarized_context[0]['summary_text']\n",
    "summarized_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels[model.predict([(claim, summarized_context)]).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels[model.predict([claim, test[4]]).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = 'COVID-19 vaccine causes infertility'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([claim, summarized_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = summarizer(context, min_length=10, max_length=128)[0]['summary_text']\n",
    "sentences = summarized_text.split('.')\n",
    "sentences = [sent for sent in sentences if len(sent.split(' '))>3]\n",
    "\n",
    "print(claim)\n",
    "print(snli_labels[model.predict([(claim, summarized_text)]).argmax(axis=1)[0]]+'\\n')\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    print(snli_labels[model.predict([(claim, sent)]).argmax(axis=1)[0]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels[model.predict([('COVID-19 vaccine does not cause infertility', 'COVID-19 vaccine causes infertility')]).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "qa_model = pipeline(model=\"andi611/distilbert-base-uncased-qa-boolq\")\n",
    "context = f\"Focus on the following statement: {claim}. Based purely on the following information, the statement is correct. {evidence.text[0]}.\"\n",
    "conclusion = qa_model(context)\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/snli\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "result = model.predict([(claim, summarized_context)])\n",
    "print(result)\n",
    "snli_labels[result.argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import BigBirdForNaturalQuestions, BigBirdTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_id = \"vasudevgupta/bigbird-roberta-natural-questions\"\n",
    "bigbird = BigBirdForQuestionAnswering.from_pretrained(model_id).to(device)\n",
    "tokenizer = BigBirdTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def get_answer(question, context):\n",
    "\n",
    "    encoding = tokenizer(question, context, return_tensors=\"pt\", max_length=256, padding=\"max_length\", truncation=True)\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    attention_mask = encoding.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_scores, end_scores = bigbird(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
    "\n",
    "    # Let's take the most likely token using `argmax` and retrieve the answer\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
    "\n",
    "    answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
    "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "    \n",
    "question = f\"Is the following claim false? {claim}\"\n",
    "get_answer(question, evidence.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zero_shot_model = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim1 = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "claim2 = 'documents were found in the home of Biden'\n",
    "\n",
    "conclusion = oracle(claim2, candidate_labels=[f\"{claim1} is true\", f\"{claim1} is false\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_labels = [f\"{claim1} is {str(bool(0))}\", f\"{claim1} is {str(bool(1))}\"]\n",
    "\n",
    "claim1 = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "claim2 = 'documents were not found in the home of Biden'\n",
    "\n",
    "conclusion = oracle(claim2, candidate_labels=temp_labels)\n",
    "bool(np.argmax(conclusion['scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"{evidence.text[0]}.\"\n",
    "conclusion = oracle(context, candidate_labels=[f\"{claim} is true\", f\"{claim} is false\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"{evidence.text[0]}.\"\n",
    "conclusion = oracle(context, candidate_labels=[f\"{claim} is false\", \n",
    "                                               f\"{claim} is true\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"Claim: COVID-19 vaccine does not cause infertility. Information: {summarized_context}.\"\n",
    "conclusion = oracle(context, candidate_labels=[\"False claim\", \"True claim\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"Claim: {claim}. Information: {summarized_context}.\"\n",
    "conclusion = oracle(context, candidate_labels=[\"False claim\", \"True claim\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim = 'COVID-19 vaccine causes infertility'\n",
    "# for ev in evidence['text'].values:\n",
    "#     context = f\"{claim}. This is logically supported by the following passages. {ev}\"\n",
    "#     conclusion = oracle(context, candidate_labels=[\"true\", \"unsure\", \"false\"])\n",
    "#     print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(model=\"gpt2\", max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(f\"{summarized_context}. Therefore, is the claim '{claim}' true or false?\", do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bevindingen\n",
    "\n",
    "- NLI is te streng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-team-tulip",
   "language": "python",
   "name": "venv-team-tulip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
