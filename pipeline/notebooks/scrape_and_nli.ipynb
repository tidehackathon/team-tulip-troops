{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../src\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import requests\n",
    "import pandas as pd\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "import get_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_evidence(parsed_evidence):\n",
    "    return ' '.join([' '.join([' '.join([el if len(el)>2 else '' for el in sl]).strip() for sl in lst]) for lst in parsed_evidence]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_evidence(claim, num_results):\n",
    "    links = get_evidences.get_top_k_results_from_google(claim, k=num_results)\n",
    "    evidence = [(l, flatten_evidence(get_evidences.get_relevant_text_from_webpage(l))) for l in links]\n",
    "\n",
    "    # Remove empty evidence\n",
    "    evidence = [ev for ev in evidence if len(ev[1]) > 0]\n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conclusions(conclusions):\n",
    "    num_sources = len(conclusions)\n",
    "    \n",
    "    false_score = conclusions.count('false')*-1\n",
    "    unsure_score = conclusions.count('unsure')*-0.2\n",
    "    true_score = conclusions.count('true')*1\n",
    "    \n",
    "    total_score = false_score + unsure_score + true_score\n",
    "    \n",
    "    print(f'Conclusions: {conclusions}')\n",
    "    print(f'Computed score: {total_score}\\n')\n",
    "    \n",
    "    if total_score < -(num_sources / 3):\n",
    "        print('This claim is very likely to be false.')\n",
    "    elif total_score < (num_sources / 3):\n",
    "        print('This claim is probably untrue.')\n",
    "    else:\n",
    "        print('This claim is plausible.')\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_claim(claim, model):\n",
    "    label_mapping = ['false', 'true', 'unsure']\n",
    "    \n",
    "    try:\n",
    "        evidence = load_evidence(claim)\n",
    "        print('Existing evidence results found locally, loading from disk.')\n",
    "    except:\n",
    "        evidence = collect_evidence(claim, num_results=10)\n",
    "        save_evidence(evidence, claim)\n",
    "        \n",
    "    conclusions = []\n",
    "\n",
    "    for i, row in evidence.iterrows():\n",
    "        source, text = row[0], row[1]\n",
    "        scores = model.predict([(claim, text)])\n",
    "\n",
    "        #Convert scores to labels\n",
    "        labels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n",
    "        conclusions += labels\n",
    "\n",
    "    parse_conclusions(conclusions)\n",
    "    evidence['conclusion'] = conclusions\n",
    "    \n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evidence(evidence, claim):\n",
    "    claim = claim.replace(' ', '').strip()\n",
    "    pd.DataFrame(evidence).to_csv(f\"../data/temp/{claim}.csv\")\n",
    "    \n",
    "def load_evidence(claim):\n",
    "    claim = claim.replace(' ', '').strip()\n",
    "    evidence = pd.read_csv(f\"../data/temp/{claim}.csv\")\n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform NLI on evidence of several claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/cross-encoder/nli-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder('cross-encoder/nli-roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brad pitt married to Britney Spears? (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"brad pitt is to marry with britney spears\"\n",
    "evidence =  = investigate_claim(claim, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joe Biden classified documents found? (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "evidence = investigate_claim(claim, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 vaccine causes infertility (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"COVID-19 vaccine causes infertility\"\n",
    "evidence = investigate_claim(claim, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snli_labels = ['true', 'unsure', 'false']\n",
    "snli_labels = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = 'COVID-19 vaccine causes infertility'\n",
    "evidence = load_evidence(claim)\n",
    "context = evidence.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarized_context = summarizer(context, min_length=5, max_length=50)\n",
    "summarized_context = summarized_context[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "claim = 'COVID-19 vaccine does not cause infertility'\n",
    "for ev in evidence['text'].values:\n",
    "    result = model.predict([(claim, context)])\n",
    "    print(result)\n",
    "    print(snli_labels[result.argmax(axis=1)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "claim = 'COVID-19 vaccine causes infertility'\n",
    "for ev in evidence['text'].values:\n",
    "    result = model.predict([(claim, context)])\n",
    "    print(result)\n",
    "    print(snli_labels[result.argmax(axis=1)[0]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "qa_model = pipeline(model=\"andi611/distilbert-base-uncased-qa-boolq\")\n",
    "context = f\"Focus on the following statement: {claim}. Based purely on the following information, the statement is correct. {evidence.text[0]}.\"\n",
    "conclusion = qa_model(context)\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/snli\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "result = model.predict([(\"This church choir sings to the masses as they sing joyous songs from the book at a church.\", \n",
    "                         \"The church is filled with song.\")])\n",
    "snli_labels[result.argmax(axis=1)[0]]\n",
    "\n",
    "# should be entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "result = model.predict([(\"A woman with a green headscarf, blue shirt and a very big grin.\", \n",
    "                         \"The woman is very happy.\")])\n",
    "snli_labels[result.argmax(axis=1)[0]]\n",
    "\n",
    "# should be entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/snli\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "result = model.predict([(claim, summarized_context)])\n",
    "print(result)\n",
    "snli_labels[result.argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import BigBirdForNaturalQuestions, BigBirdTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_id = \"vasudevgupta/bigbird-roberta-natural-questions\"\n",
    "bigbird = BigBirdForQuestionAnswering.from_pretrained(model_id).to(device)\n",
    "tokenizer = BigBirdTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def get_answer(question, context):\n",
    "\n",
    "    encoding = tokenizer(question, context, return_tensors=\"pt\", max_length=256, padding=\"max_length\", truncation=True)\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    attention_mask = encoding.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_scores, end_scores = bigbird(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
    "\n",
    "    # Let's take the most likely token using `argmax` and retrieve the answer\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
    "\n",
    "    answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
    "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "    \n",
    "question = f\"Is the following claim false? {claim}\"\n",
    "get_answer(question, evidence.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "oracle = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"{evidence.text[0]}.\"\n",
    "conclusion = oracle(context, candidate_labels=[f\"{claim} is true\", f\"{claim} is false\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"{evidence.text[0]}.\"\n",
    "conclusion = oracle(context, candidate_labels=[f\"{claim} is false\", \n",
    "                                               f\"{claim} is true\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"Claim: COVID-19 vaccine does not cause infertility. Information: {summarized_context}.\"\n",
    "conclusion = oracle(context, candidate_labels=[\"False claim\", \"True claim\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"Claim: {claim}. Information: {summarized_context}.\"\n",
    "conclusion = oracle(context, candidate_labels=[\"False claim\", \"True claim\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim = 'COVID-19 vaccine causes infertility'\n",
    "# for ev in evidence['text'].values:\n",
    "#     context = f\"{claim}. This is logically supported by the following passages. {ev}\"\n",
    "#     conclusion = oracle(context, candidate_labels=[\"true\", \"unsure\", \"false\"])\n",
    "#     print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(model=\"gpt2\", max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(f\"{summarized_context}. Therefore, is the claim '{claim}' true or false?\", do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-tulip-troops",
   "language": "python",
   "name": "venv-tulip-troops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
