{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../src\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "requests.packages.urllib3.disable_warnings() \n",
    "\n",
    "import get_evidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_evidence(parsed_evidence):\n",
    "    return ' '.join([' '.join([' '.join([el if len(el)>2 else '' for el in sl]).strip() for sl in lst]) for lst in parsed_evidence]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    global summarizer\n",
    "    \n",
    "    # truncate input somewhat (due to model max length)\n",
    "    # TODO split context into chunks of max len, concat later?\n",
    "    maxlen = 700\n",
    "    if type(text) == str:\n",
    "        text = ' '.join([txt for txt in text.split(' ')[:maxlen]])\n",
    "    else:\n",
    "        text = [' '.join([txt for txt in subtext.split(' ')[:maxlen]]) for subtext in text]\n",
    "\n",
    "        summarized_text = summarizer(text, min_length=min(10, len(text)), max_length=(min(128, len(text))))\n",
    "    \n",
    "    if type(text) != str:\n",
    "        summarized_text = [txt['summary_text'] for txt in summarized_text]\n",
    "    else:\n",
    "        summarized_text = summarized_text[0]['summary_text']\n",
    "        \n",
    "    return summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_evidence(claim, num_results):\n",
    "    try:\n",
    "        evidence = load_evidence(claim)\n",
    "        print('Existing evidence results found locally, loading from disk.')\n",
    "    except:\n",
    "        links = get_evidences.get_top_k_results_from_google(claim, k=num_results)\n",
    "        evidence = [(l, flatten_evidence(get_evidences.get_relevant_text_from_webpage(l))) for l in links]\n",
    "\n",
    "        # Remove empty evidence\n",
    "        evidence = pd.DataFrame([ev for ev in evidence if len(ev[1]) > 0], columns=['source', 'text'])\n",
    "        \n",
    "        # Clean input\n",
    "        evidence['text'] = evidence['text'].apply(lambda x: clean_input(x))\n",
    "    \n",
    "        # Remove duplicates\n",
    "        evidence.drop_duplicates('text', inplace=True)\n",
    "\n",
    "        # Add summarization\n",
    "        evidence['summary'] = summarize_text(list(evidence['text'].values))\n",
    "\n",
    "        # Save for later use\n",
    "        save_evidence(evidence, claim)\n",
    "        \n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conclusions(conclusions):\n",
    "    num_sources = len(conclusions)\n",
    "    \n",
    "    false_score = conclusions.count('false')*-1\n",
    "    unsure_score = conclusions.count('unsure')*-0.2\n",
    "    true_score = conclusions.count('true')*1\n",
    "    \n",
    "    total_score = false_score + unsure_score + true_score\n",
    "    \n",
    "    print(f'Conclusions: {conclusions}')\n",
    "    print(f'Computed score: {total_score}\\n')\n",
    "    \n",
    "    if total_score < -(num_sources / 3):\n",
    "        print('This claim is very likely to be false.')\n",
    "    elif total_score < (num_sources / 3):\n",
    "        print('This claim is probably untrue.')\n",
    "    else:\n",
    "        print('This claim is plausible.')\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(text):\n",
    "    # Remove newline\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "def split_to_sentences(text):\n",
    "    return sent_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_zero_shot_model(claim, text):\n",
    "#     global zero_shot_model\n",
    "#     unsure_threshold = 0.8\n",
    "#     prefix = 'This tentatively proves that'\n",
    "#     temp_labels = [f\"{prefix} the claim '{claim}' is {str(bool(0))}\",\n",
    "#                    'This proves nothing',\n",
    "#                    f\"{prefix} the claim '{claim}' is {str(bool(1))}\"]\n",
    "\n",
    "#     conclusion = zero_shot_model(text, candidate_labels=temp_labels)\n",
    "#     true_or_false = conclusion['labels'][0].split(' ')[-1].lower() == 'true'\n",
    "#     score = conclusion['scores'][0]\n",
    "    \n",
    "# #     print(conclusion)\n",
    "# #     print(true_or_false)\n",
    "# #     print(score)\n",
    "    \n",
    "    \n",
    "#     return str(true_or_false).lower() if score > unsure_threshold else 'unsure'\n",
    "\n",
    "\n",
    "# print(apply_zero_shot_model(\"brad pitt is to marry with britney spears\", evidence.summary.values[0]))\n",
    "# print('should be False\\n')\n",
    "# print(apply_zero_shot_model(\"brad pitt going to marry with britney spears\", evidence.summary.values[0]))\n",
    "# print('should be False\\n')\n",
    "# print(apply_zero_shot_model(\"brad pitt is not to marry with britney spears\", evidence.summary.values[0]))\n",
    "# print('should be True\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nli_model(claim, text):\n",
    "    global nli_model\n",
    "    label_mapping = ['false', 'true', 'unsure']   # (['contradiction', 'entailment', 'neutral'])\n",
    "\n",
    "    scores = nli_model.predict([(claim, text)])\n",
    "    \n",
    "    #Convert scores to labels\n",
    "    labels = [label_mapping[score_max] for score_max in scores.argmax(axis=1)]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate_claim(claim):\n",
    "    \n",
    "    evidence = collect_evidence(claim, num_results=10)\n",
    "        \n",
    "    conclusions = []\n",
    "    for i, row in evidence.iterrows():\n",
    "        source, text = row[0], row[1]\n",
    "        labels = apply_nli_model(claim, text)\n",
    "        conclusions += labels\n",
    "\n",
    "    parse_conclusions(conclusions)\n",
    "    evidence['conclusion'] = conclusions\n",
    "    \n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_evidence(evidence, claim):\n",
    "    claim = claim.replace(' ', '').strip()\n",
    "    pd.DataFrame(evidence).to_csv(f\"../data/temp/{claim}.csv\", sep ='\\t')\n",
    "    \n",
    "def load_evidence(claim):\n",
    "    claim = claim.replace(' ', '').strip()\n",
    "    evidence = pd.read_csv(f\"../data/temp/{claim}.csv\", sep ='\\t')\n",
    "    return evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform NLI on evidence of several claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/cross-encoder/nli-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_model.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "zero_shot_model = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brad pitt married to Britney Spears? (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"brad pitt is to marry with britney spears\"\n",
    "evidence = investigate_claim(claim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "claim = \"brad pitt is to marry with britney spears\"\n",
    "\n",
    "for summ in evidence.summary.values:\n",
    "    conclusion = apply_zero_shot_model(claim, s)\n",
    "    print(summ)\n",
    "    print(conclusion)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joe Biden classified documents found? (true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "claim = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "evidence = investigate_claim(claim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "claim = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "\n",
    "for summ in evidence.summary.values:\n",
    "    conclusion = apply_zero_shot_model(claim, summ)\n",
    "    print(summ)\n",
    "    print(conclusion)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 vaccine causes infertility (fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = \"COVID-19 vaccine causes infertility\"\n",
    "evidence = investigate_claim(claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels = ['contradiction', 'entailment', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = 'COVID-19 vaccine causes infertility'\n",
    "evidence = load_evidence(claim)\n",
    "context = evidence.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_context = summarizer(context, min_length=5, max_length=50)\n",
    "summarized_context = summarized_context[0]['summary_text']\n",
    "summarized_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels[model.predict([(claim, summarized_context)]).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels[model.predict([claim, test[4]]).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim = 'COVID-19 vaccine causes infertility'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([claim, summarized_context])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarized_text = summarizer(context, min_length=10, max_length=128)[0]['summary_text']\n",
    "sentences = summarized_text.split('.')\n",
    "sentences = [sent for sent in sentences if len(sent.split(' '))>3]\n",
    "\n",
    "print(claim)\n",
    "print(snli_labels[model.predict([(claim, summarized_text)]).argmax(axis=1)[0]]+'\\n')\n",
    "for sent in sentences:\n",
    "    print(sent)\n",
    "    print(snli_labels[model.predict([(claim, sent)]).argmax(axis=1)[0]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_labels[model.predict([('COVID-19 vaccine does not cause infertility', 'COVID-19 vaccine causes infertility')]).argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "qa_model = pipeline(model=\"andi611/distilbert-base-uncased-qa-boolq\")\n",
    "context = f\"Focus on the following statement: {claim}. Based purely on the following information, the statement is correct. {evidence.text[0]}.\"\n",
    "conclusion = qa_model(context)\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/snli\n",
    "model = CrossEncoder('cross-encoder/nli-distilroberta-base')\n",
    "\n",
    "result = model.predict([(claim, summarized_context)])\n",
    "print(result)\n",
    "snli_labels[result.argmax(axis=1)[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import BigBirdForNaturalQuestions, BigBirdTokenizerFast\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model_id = \"vasudevgupta/bigbird-roberta-natural-questions\"\n",
    "bigbird = BigBirdForQuestionAnswering.from_pretrained(model_id).to(device)\n",
    "tokenizer = BigBirdTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def get_answer(question, context):\n",
    "\n",
    "    encoding = tokenizer(question, context, return_tensors=\"pt\", max_length=256, padding=\"max_length\", truncation=True)\n",
    "    input_ids = encoding.input_ids.to(device)\n",
    "    attention_mask = encoding.attention_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_scores, end_scores = bigbird(input_ids=input_ids, attention_mask=attention_mask).to_tuple()\n",
    "\n",
    "    # Let's take the most likely token using `argmax` and retrieve the answer\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"][0].tolist())\n",
    "\n",
    "    answer_tokens = all_tokens[torch.argmax(start_scores): torch.argmax(end_scores)+1]\n",
    "    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "    return answer\n",
    "    \n",
    "question = f\"Is the following claim false? {claim}\"\n",
    "get_answer(question, evidence.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zero_shot_model = pipeline(model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim1 = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "claim2 = 'documents were found in the home of Biden'\n",
    "\n",
    "conclusion = oracle(claim2, candidate_labels=[f\"{claim1} is true\", f\"{claim1} is false\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_labels = [f\"{claim1} is {str(bool(0))}\", f\"{claim1} is {str(bool(1))}\"]\n",
    "\n",
    "claim1 = \"Joe Biden took home classified documents after leaving the vice-presidency\"\n",
    "claim2 = 'documents were not found in the home of Biden'\n",
    "\n",
    "conclusion = oracle(claim2, candidate_labels=temp_labels)\n",
    "bool(np.argmax(conclusion['scores']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"{evidence.text[0]}.\"\n",
    "conclusion = oracle(context, candidate_labels=[f\"{claim} is true\", f\"{claim} is false\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"{evidence.text[0]}.\"\n",
    "conclusion = oracle(context, candidate_labels=[f\"{claim} is false\", \n",
    "                                               f\"{claim} is true\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"Claim: COVID-19 vaccine does not cause infertility. Information: {summarized_context}.\"\n",
    "conclusion = oracle(context, candidate_labels=[\"False claim\", \"True claim\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"Claim: {claim}. Information: {summarized_context}.\"\n",
    "conclusion = oracle(context, candidate_labels=[\"False claim\", \"True claim\"])\n",
    "print(conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim = 'COVID-19 vaccine causes infertility'\n",
    "# for ev in evidence['text'].values:\n",
    "#     context = f\"{claim}. This is logically supported by the following passages. {ev}\"\n",
    "#     conclusion = oracle(context, candidate_labels=[\"true\", \"unsure\", \"false\"])\n",
    "#     print(conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(model=\"gpt2\", max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator(f\"{summarized_context}. Therefore, is the claim '{claim}' true or false?\", do_sample=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bevindingen\n",
    "\n",
    "- NLI is te streng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-team-tulip",
   "language": "python",
   "name": "venv-team-tulip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
